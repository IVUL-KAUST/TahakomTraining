{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tahakom Training Week 3: Beyond-supervised-training\n",
    "##  Day1: Self-Supervised Learning with Rotation Prediction\n",
    "\n",
    "Today, we will explore the concepts of self-supervised learning using Rotation Prediction task.  During this practical session we will set up the development environment, implement its framework, pretraining and evaluation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Description: Rotation Prediction\n",
    "## Background\n",
    "Self-Supervised Learning (SSL) is a technique that leverages the inherent structure in data to learn useful representations without the need for large amounts of labeled data. In computer vision tasks, SSL uses pretext tasks to train models, enabling them to extract meaningful information from unlabeled data. This approach is particularly valuable in visual tasks as it improves the model’s generalization ability for downstream tasks. \n",
    "Rotation prediction is a widely used pretext task in SSL, first introduced by Gidaris et al. in 2018. The core idea behind this task is to take an image, apply random rotations (e.g., 0°, 90°, 180°, and 270°), and require the model to predict the rotation angle. This forces the model to learn the semantic information and geometric structure of the objects in the image, resulting in more effective visual feature learning.\n",
    "Framework of [Rotation Prediction](https://arxiv.org/abs/1803.07728) is visualized below.\n",
    "\n",
    "<center width=\"100%\"><img src=\"https://github.com/IVUL-KAUST/TahakomTraining/blob/main/Week-3-SSL/Notebooks/Day-1-Intro-SSL/rotnet.png?raw=true\" width=\"500px\"></center>\n",
    "\n",
    "\n",
    "## Task Definition\n",
    "In this assignment, we will use the rotation prediction task as a pre-training objective in a self-supervised learning framework. Specifically, we will apply four possible rotation transformations to the input images:\n",
    "\n",
    "0°: No rotation, the image remains unchanged.\n",
    "\n",
    "90°: Rotate the image 90 degrees clockwise.\n",
    "\n",
    "180°: Rotate the image 180 degrees clockwise.\n",
    "\n",
    "270°: Rotate the image 270 degrees clockwise.\n",
    "\n",
    "The goal is to design a model that takes in the rotated image and predicts the corresponding rotation angle class (0, 90, 180, or 270)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup\n",
    "\n",
    "The first thing to do is implement a dataset class to load rotated CIFAR10 images with matching labels. Since there is already a CIFAR10 dataset class implemented in `torchvision`, we will extend this class and modify the `__get_item__` method appropriately to load rotated images.\n",
    "\n",
    "Each rotation label should be an integer in the set {0, 1, 2, 3} which correspond to rotations of 0, 90, 180, or 270 degrees respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "Show some example images with labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Denormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = torch.tensor(mean).view(3, 1, 1)\n",
    "        self.std = torch.tensor(std).view(3, 1, 1)\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        return tensor * self.std + self.mean\n",
    "\n",
    "mean = (0.4914, 0.4822, 0.4465)\n",
    "std = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "denormalize = Denormalize(mean, std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 5\n",
    "images, labels = zip(*[(test_dataset[i][0], test_dataset[i][1]) for i in range(num_images)])\n",
    "\n",
    "def show_tensor_images(images, labels, classes):\n",
    "    fig, axes = plt.subplots(1, len(images), figsize=(12, 3))\n",
    "    for i, image in enumerate(images):\n",
    "        image = denormalize(image)\n",
    "        image = image.permute(1, 2, 0).numpy()  \n",
    "        image = np.clip(image * 255, 0, 255).astype(np.uint8)\n",
    "        axes[i].imshow(image)\n",
    "        axes[i].set_title(classes[labels[i]])\n",
    "        axes[i].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "show_tensor_images(images, labels, test_dataset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "For this task, the CIFAR-100 dataset will be loaded and prepared using deep learning frameworks such as PyTorch. The images will be normalized, and during preprocessing, we will apply rotations of 0°, 90°, 180°, and 270° to create the data required for the rotation prediction task. Write the data loading function to return the necessary rotated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "def rotate_image(image, rotation):\n",
    "    if rotation == 90:\n",
    "        return torch.rot90(image, 1, [1, 2])\n",
    "    elif rotation == 180:\n",
    "        return torch.rot90(image, 2, [1, 2])\n",
    "    elif rotation == 270:\n",
    "        return torch.rot90(image, 3, [1, 2])\n",
    "    else:\n",
    "        return image\n",
    "\n",
    "class RotationPredictionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, seed=None):\n",
    "        self.dataset = dataset\n",
    "        self.rotations = [0, 90, 180, 270]\n",
    "        self.labels = {0: 0, 90: 1, 180: 2, 270: 3}\n",
    "        if seed is not None:\n",
    "            random.seed(seed) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        #  Write the data set function return rotated image required for the SSL task\n",
    "        #your implementation goes here\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "Show some rotated images with labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_rotation_dataset(dataset, num_images=5):\n",
    "    images, labels = zip(*[dataset[i] for i in range(num_images)])\n",
    "\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(12, 3))\n",
    "    for i, image in enumerate(images):\n",
    "        image = denormalize(image)\n",
    "        image = image.permute(1, 2, 0).numpy()  \n",
    "        image = np.clip(image * 255, 0, 255).astype(np.uint8)  \n",
    "        axes[i].imshow(image)\n",
    "        axes[i].set_title(f\"Rotation: {labels[i] * 90}°\")  \n",
    "        axes[i].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_dataset = RotationPredictionDataset(train_dataset)\n",
    "visualize_rotation_dataset(pretrain_dataset)\n",
    "pretrain_test_dataset = RotationPredictionDataset(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def eval(model, model_path, dataset, batch_size, device):\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, init_lr, decay_epochs=30):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = init_lr * (0.1 ** (epoch // decay_epochs))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, loss_fn, optimizer, epochs, save_path, device, init_lr, decay_epochs):\n",
    "    model.train()  \n",
    "    best_loss = float('inf')\n",
    "    epoch_losses = []  \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            adjust_learning_rate(optimizer, epoch, init_lr, decay_epochs)\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        epoch_losses.append(avg_loss)  \n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}')\n",
    "\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f'Model is saved to {save_path}, Best Loss: {best_loss:.4f}')\n",
    "\n",
    "    print(\"Training finished!\")\n",
    "    return epoch_losses\n",
    "\n",
    "def plot_loss_curve(losses):\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, len(losses) + 1), losses, marker='o')\n",
    "    plt.title(\"Training Loss Curve\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretraining\n",
    "For this practical session, we will train a ResNet18 model on the rotation task. The input is a rotated image and the model predicts the rotation label. See the Data Setup section for details. We will randomly initialize a ResNet18 model and pretrain on the rotation prediction task. Your task is to instantiate the correct the model and loss function required for the task of rotation prediction. You can directly use inbuilt pytorch modules for that. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "model = None # To do, Create the required model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "loss_fn = # To do, Write your loss here\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 0\n",
    "learning_rate = 0.01\n",
    "decay_epochs = 15\n",
    "best_save_path = \"./best_pretrained_model.pth\"\n",
    "last_save_path = \"./last_pretrained_model.pth\"\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_loader = DataLoader(pretrain_dataset, batch_size=batch_size, shuffle=True)\n",
    "epoch_losses = train_model(model, train_loader, loss_fn, optimizer, epochs, best_save_path, device, init_lr=learning_rate, decay_epochs=decay_epochs)\n",
    "\n",
    "plot_loss_curve(epoch_losses)\n",
    "torch.save(model.state_dict(), last_save_path)\n",
    "eval(model, last_save_path, pretrain_dataset, batch_size, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "There are two way to evaluate features that we learn from self-supervised pertaining. \n",
    "(1) Finetuning: Initialize the model with the pretrained weights from SSL tasks and finetune the whole network  for the downstream task.\n",
    "(2) Linear Classification: Initialize the model with the pretrained weights from SSL tasks and  freeze the backbone network and train a linear classifier only the for the downstream task. Next, we are going to implement both evaluations for the model that we trained above using SSL pretraining. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning\n",
    "In this section, we will load the pretrained weight and fine-tune on the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "# your implementation goes here\n",
    "# note that you have to load the weights from the model that we trained above and train all its layers for the task of classification\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "learning_rate = 0.01\n",
    "decay_epochs = 10\n",
    "best_save_path = \"./best_finetune_pretrained_model.pth\"\n",
    "last_save_path = \"./last_finetune_pretrained_model.pth\"\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epoch_losses = train_model(model, train_loader, loss_fn, optimizer, epochs, best_save_path, device, learning_rate, decay_epochs=decay_epochs)\n",
    "plot_loss_curve(epoch_losses)\n",
    "torch.save(model.state_dict(), last_save_path)\n",
    "eval(model, last_save_path, test_dataset, batch_size, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear evaluation\n",
    "In this section, we will load the pretrained weight and train for the classification task. We will freeze all previous layers except for the 'layer4' block and 'fc' layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "# your implementation goes here\n",
    "# note that you have to freeze the backbone network layers except for the 'layer4' block and 'fc' layer.\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "learning_rate = 0.01\n",
    "decay_epochs = 10\n",
    "best_save_path = \"./best_linear_pretrained_model.pth\"\n",
    "last_save_path = \"./last_linear_pretrained_model.pth\"\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epoch_losses = train_model(model, train_loader, loss_fn, optimizer, epochs, best_save_path, device, learning_rate, decay_epochs=decay_epochs)\n",
    "plot_loss_curve(epoch_losses)\n",
    "torch.save(model.state_dict(), last_save_path)\n",
    "eval(model, last_save_path, test_dataset, batch_size, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised training from scratch\n",
    "In this section, we will randomly initialize a ResNet18 model and train it directly  on the target dataset for classification task. We can directly compare the performance with the finetuning above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18(num_classes=10)\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "learning_rate = 0.01\n",
    "decay_epochs = 10\n",
    "best_save_path = \"./best_supervised_pretrained_model.pth\"\n",
    "last_save_path = \"./last_supervised_pretrained_model.pth\"\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epoch_losses = train_model(model, train_loader, loss_fn, optimizer, epochs, best_save_path, device, init_lr=learning_rate, decay_epochs=decay_epochs)\n",
    "plot_loss_curve(epoch_losses)\n",
    "torch.save(model.state_dict(), last_save_path)\n",
    "eval(model, last_save_path, test_dataset, batch_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
