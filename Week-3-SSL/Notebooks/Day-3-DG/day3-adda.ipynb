{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Introduction\n","\n","Adversarial Discriminative Domain Adaptation (ADDA)\n","\n","In real-world applications, machine learning models are often trained on a dataset that is not perfectly representative of the domain where the model will be deployed. This difference between the source domain (where the model is trained) and the target domain (where the model is applied) is called **domain shift**, and it often leads to a significant drop in performance when models trained on the source domain are tested on the target domain.\n","\n","<div style=\"text-align:center\">\n","<img src=\"https://i.ibb.co/v3x80W9/Screenshot-2024-10-22-at-11-24-32-AM.png\" alt=\"Adversarial Discriminative Domain Adaptation\" border=\"0\" width=400>\n","</div>\n","\n","Adversarial Discriminative Domain Adaptation (ADDA) is a technique designed to address this problem through unsupervised domain adaptation. The key idea is to transfer the knowledge learned from a labeled source domain to an unlabeled target domain by aligning the feature distributions between the two domains, See above Figure. ADDA achieves this through a two-step process:\n","\n","1. Feature learning on the source domain: A feature extractor is trained on the labeled source domain in a supervised manner.\n","2. Adversarial adaptation for the target domain: The model then uses adversarial training to align the target domain features with those from the source domain, ensuring that the feature representations are domain-invariant.\n","\n","The method is inspired by the concept of Generative Adversarial Networks (GANs), where a domain discriminator attempts to distinguish between source and target domain features, while the target feature extractor learns to fool the discriminator by producing features that are indistinguishable between domains.\n","\n","Objectives:\n","\n","- Learn domain-invariant features: Align the feature spaces of the source and target domains without requiring labels from the target domain.\n","- Improve generalization: Train a model that performs well on the target domain despite the domain shift.\n","\n","This notebook will guide you through the implementation of ADDA, covering key steps such as:\n","\n","- Pretraining a feature extractor on the source domain.\n","- Applying adversarial training for domain adaptation.\n","- Evaluating the performance on the target domain.\n","\n","By the end of this notebook, you will understand how adversarial techniques can be used to adapt a model to a new, unlabeled domain.\n"]},{"cell_type":"markdown","metadata":{},"source":["## Model Overview\n","\n","<div style=\"text-align:center\">\n","    <img src=\"https://i.ibb.co/2sCRxmF/Screenshot-2024-10-22-at-11-29-59-AM.png\" alt=\"Screenshot-2024-10-22-at-11-29-59-AM\" border=\"0\" width=800>\n","</div>\n","\n","Paper: https://arxiv.org/pdf/1702.05464\n","\n","#### NOTE:\n","**Here we have MNIST as Source images and MNIST-M as Target**"]},{"cell_type":"markdown","metadata":{},"source":["## Pretraining a feature extractor on the source domain (MNIST)"]},{"cell_type":"markdown","metadata":{},"source":["### Libraries ðŸ“šâ¬‡"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from PIL import Image\n","from pathlib import Path\n","import random, math, cv2\n","from tqdm import tqdm_notebook as tqdm\n","\n","import torch\n","from torch import nn\n","from torchvision.datasets import MNIST\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import datasets, transforms\n","from torchvision.transforms import Compose, ToTensor\n","\n","import matplotlib.pyplot as plt\n","import plotly\n","import plotly.express as px\n","import plotly.graph_objects as go\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def visualize_digits(dataset, k=80, mnistm=False, cmap=None, title=None):\n","    \n","    ncols = 20\n","    indices = random.choices(range(len(dataset)), k=k)\n","    nrows = math.floor(len(indices)/ncols)\n","    \n","    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols,nrows+0.4), gridspec_kw=dict(wspace=0.1, hspace=0.1), subplot_kw=dict(yticks=[], xticks=[]))\n","    axes_flat = axes.reshape(-1)\n","    fig.suptitle(title, fontsize=20)\n","    \n","    for list_idx, image_idx in enumerate(indices[:ncols*nrows]):\n","        ax = axes_flat[list_idx]\n","        image = dataset[image_idx][0]\n","        image = image.numpy().transpose(1, 2, 0)\n","        ax.imshow(image, cmap=cmap)\n","\n","def set_requires_grad(model, requires_grad=True):\n","    for param in model.parameters():\n","        param.requires_grad = requires_grad\n","\n","def loop_iterable(iterable):\n","    while True:\n","        yield from iterable\n","\n","class GrayscaleToRgb:\n","    \"\"\"Convert a grayscale image to rgb\"\"\"\n","    def __call__(self, image):\n","        image = np.array(image)\n","        image = np.dstack([image, image, image])\n","        return Image.fromarray(image)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["MNIST_DATA_DIR = Path('/kaggle/working')\n","BSDS_DATA_DIR = Path('/kaggle/working/bsds500')\n","MODEL_FILE = Path('best_source_weights_mnist.pth')\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Hyperparameters\n","batch_size = 64\n","epochs = 5\n"]},{"cell_type":"markdown","metadata":{},"source":["### Download the data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!mkdir -p mnist/MNIST/raw/\n","!wget https://raw.githubusercontent.com/fgnt/mnist/master/train-images-idx3-ubyte.gz\n","!wget https://raw.githubusercontent.com/fgnt/mnist/master/train-labels-idx1-ubyte.gz\n","!wget https://raw.githubusercontent.com/fgnt/mnist/master/t10k-images-idx3-ubyte.gz\n","!wget https://raw.githubusercontent.com/fgnt/mnist/master/t10k-labels-idx1-ubyte.gz\n","!mv train-images-idx3-ubyte.gz /kaggle/working/mnist/MNIST/raw/\n","!mv train-labels-idx1-ubyte.gz /kaggle/working/mnist/MNIST/raw/\n","!mv t10k-images-idx3-ubyte.gz /kaggle/working/mnist/MNIST/raw/\n","!mv t10k-labels-idx1-ubyte.gz /kaggle/working/mnist/MNIST/raw/"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!curl -L -o bsds500.zip https://www.kaggle.com/api/v1/datasets/download/balraj98/berkeley-segmentation-dataset-500-bsds500\n","!mkdir bsds500\n","!unzip -q bsds500.zip -d bsds500/\n","!rm bsds500.zip"]},{"cell_type":"markdown","metadata":{},"source":["### Define Model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.feature_extractor = nn.Sequential(\n","            nn.Conv2d(3, 10, kernel_size=5),\n","            nn.MaxPool2d(2),\n","            nn.ReLU(),\n","            nn.Conv2d(10, 20, kernel_size=5),\n","            nn.MaxPool2d(2),\n","            nn.Dropout2d(),\n","        )\n","        \n","        self.classifier = nn.Sequential(\n","            nn.Linear(320, 50),\n","            nn.ReLU(),\n","            nn.Dropout(),\n","            nn.Linear(50, 10),\n","            nn.LogSoftmax(),\n","        )\n","\n","    def forward(self, x):\n","        features = self.feature_extractor(x)\n","        features = features.view(x.shape[0], -1)\n","        logits = self.classifier(features)\n","        return logits"]},{"cell_type":"markdown","metadata":{},"source":["### Get Dataset & Dataloaders"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["source_model = Net().to(device)\n","if MODEL_FILE.exists():\n","    source_model.load_state_dict(torch.load(MODEL_FILE))\n","\n","train_dataset = MNIST(MNIST_DATA_DIR / 'mnist', train=True, download=True, transform=Compose([GrayscaleToRgb(), ToTensor()]))\n","test_dataset = MNIST(MNIST_DATA_DIR / 'mnist', train=False, download=True, transform=Compose([GrayscaleToRgb(), ToTensor()]))\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=32, pin_memory=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=32, pin_memory=True)\n","\n","source_optim = torch.optim.Adam(source_model.parameters(), lr=0.002)\n","criterion = nn.NLLLoss()"]},{"cell_type":"markdown","metadata":{},"source":["### Visualize MNIST Data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["visualize_digits(dataset=train_dataset, k=120, cmap='gray', title='Sample MNIST Images')"]},{"cell_type":"markdown","metadata":{},"source":["### Train Source Model on MNIST"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_losses, train_accuracies, train_counter = [], [], []\n","test_losses, test_accuracies = [], []\n","test_counter = [idx*len(train_loader.dataset) for idx in range(0, epochs+1)]\n","\n","def train(epoch):\n","    train_loss, train_accuracy = 0, 0\n","    source_model.train()\n","    tqdm_bar = tqdm(train_loader, desc=f'Training Epoch {epoch} ', total=int(len(train_loader)))\n","    for idx, (images, labels) in enumerate(tqdm_bar):\n","        images, labels = images.to(device), labels.to(device)\n","        source_optim.zero_grad()\n","        with torch.set_grad_enabled(True):\n","            outputs = source_model(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            source_optim.step()\n","        train_loss += loss.item()\n","        train_losses.append(loss.item())\n","        outputs = torch.argmax(outputs, dim=1).type(torch.FloatTensor).to(device)\n","        train_batch_accuracy = torch.mean((outputs == labels).type(torch.FloatTensor)).item()\n","        train_accuracy += train_batch_accuracy\n","        train_accuracies.append(train_batch_accuracy)\n","        tqdm_bar.set_postfix(train_loss=(train_loss/(idx+1)), train_accuracy=train_accuracy/(idx+1))\n","        train_counter.append(idx*batch_size + images.size(0) + epoch*len(train_dataset))\n","\n","def test():\n","    test_loss, test_accuracy = 0, 0\n","    source_model.eval()\n","    tqdm_bar = tqdm(test_loader, desc=f'Testing ', total=int(len(test_loader)))\n","    for idx, (images, labels) in enumerate(tqdm_bar):\n","        images, labels = images.to(device), labels.to(device)\n","        with torch.no_grad():\n","            outputs = source_model(images)\n","            loss = criterion(outputs, labels)\n","        test_loss += loss.item()\n","        outputs = torch.argmax(outputs, dim=1).type(torch.FloatTensor).to(device)\n","        test_accuracy += torch.mean((outputs == labels).type(torch.FloatTensor)).item()\n","        tqdm_bar.set_postfix(test_loss=(test_loss/(idx+1)), test_accuracy=test_accuracy/(idx+1))\n","    test_losses.append(test_loss/len(test_loader))\n","    test_accuracies.append(test_accuracy/len(test_loader))\n","    if np.argmax(test_accuracies) == len(test_accuracies)-1:\n","        torch.save(source_model.state_dict(), 'best_source_weights_mnist.pth')\n","        \n","test()\n","for epoch in range(epochs):\n","    train(epoch)\n","    test()"]},{"cell_type":"markdown","metadata":{},"source":["#### Visualize Training & Testing Results ðŸ“ˆ"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig = go.Figure()\n","fig.add_trace(go.Scatter(x=train_counter, y=train_losses, mode='lines', name='Train loss'))\n","fig.add_trace(go.Scatter(x=test_counter, y=test_losses, marker_symbol='star-diamond', \n","                         marker_color='orange', marker_line_width=1, marker_size=9, mode='markers', name='Test loss'))\n","fig.update_layout(\n","    width=1000,\n","    height=500,\n","    title=\"Train vs. Test Loss\",\n","    xaxis_title=\"Number of training examples seen\",\n","    yaxis_title=\"Negative Log Likelihood loss\"),\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig = go.Figure()\n","fig.add_trace(go.Scatter(x=train_counter, y=train_accuracies, mode='lines', name='Train loss'))\n","fig.add_trace(go.Scatter(x=test_counter, y=test_accuracies, marker_symbol='star-diamond', \n","                         marker_color='orange', marker_line_width=1, marker_size=9, mode='markers', name='Test Accuracy'))\n","fig.update_layout(\n","    width=1000,\n","    height=500,\n","    title=\"Train vs. Test Accuracy\",\n","    xaxis_title=\"Number of training examples seen\",\n","    yaxis_title=\"Accuracy\")\n","fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Adversarial training for domain adaptation"]},{"cell_type":"markdown","metadata":{},"source":["Now we will load the pre-trained model on source domain and apply the adversarial training for domain adaptation"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Hyperparameters\n","\n","batch_size = 64\n","iterations = 500\n","epochs = 4\n","k_disc = 1\n","k_clf = 10\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"markdown","metadata":{},"source":["`MODEL_FILE` contains the path to pretrained source model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["source_model = Net().to(device)\n","if MODEL_FILE:\n","    source_model.load_state_dict(torch.load(MODEL_FILE, map_location=device))\n","source_model.eval()\n","set_requires_grad(source_model, requires_grad=False)\n","\n","clf = source_model\n","source_model = source_model.feature_extractor\n","\n","target_model = Net().to(device)\n","if MODEL_FILE:\n","    target_model.load_state_dict(torch.load(MODEL_FILE, map_location=device))\n","target_model = target_model.feature_extractor\n","\n","discriminator = nn.Sequential(\n","    nn.Linear(320, 120),\n","    nn.ReLU(),\n","    nn.Linear(120, 20),\n","    nn.ReLU(),\n","    nn.Linear(20, 1)\n",").to(device)"]},{"cell_type":"markdown","metadata":{},"source":["### Define Dataset Classes"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class BSDS500(Dataset):\n","\n","    def __init__(self):\n","        image_folder = BSDS_DATA_DIR / 'images'\n","        self.image_files = list(map(str, image_folder.glob('*/*.jpg')))\n","\n","    def __getitem__(self, i):\n","        image = cv2.imread(self.image_files[i], cv2.IMREAD_COLOR)\n","        tensor = torch.from_numpy(image.transpose(2, 0, 1))\n","        return tensor\n","\n","    def __len__(self):\n","        return len(self.image_files)\n","\n","\n","class MNISTM(Dataset):\n","\n","    def __init__(self, train=True):\n","        super(MNISTM, self).__init__()\n","        self.mnist = datasets.MNIST(MNIST_DATA_DIR / 'mnist', train=train,\n","                                    download=True)\n","        self.bsds = BSDS500()\n","        # Fix RNG so the same images are used for blending\n","        self.rng = np.random.RandomState(42)\n","\n","    def __getitem__(self, i):\n","        digit, label = self.mnist[i]\n","        digit = transforms.ToTensor()(digit)\n","        bsds_image = self._random_bsds_image()\n","        patch = self._random_patch(bsds_image)\n","        patch = patch.float() / 255\n","        blend = torch.abs(patch - digit)\n","        return blend, label\n","\n","    def _random_patch(self, image, size=(28, 28)):\n","        _, im_height, im_width = image.shape\n","        x = self.rng.randint(0, im_width-size[1])\n","        y = self.rng.randint(0, im_height-size[0])\n","        return image[:, y:y+size[0], x:x+size[1]]\n","\n","    def _random_bsds_image(self):\n","        i = self.rng.choice(len(self.bsds))\n","        return self.bsds[i]\n","\n","    def __len__(self):\n","        return len(self.mnist)"]},{"cell_type":"markdown","metadata":{},"source":["### Get Dataset & Dataloaders"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["half_batch = batch_size // 2\n","\n","source_dataset = MNIST(MNIST_DATA_DIR/'mnist', train=True, download=True, transform=Compose([GrayscaleToRgb(), ToTensor()]))\n","source_loader = DataLoader(source_dataset, batch_size=half_batch, shuffle=True, num_workers=16, pin_memory=True)\n","\n","target_train_dataset, target_test_dataset = MNISTM(train=True), MNISTM(train=False)\n","target_train_loader = DataLoader(target_train_dataset, batch_size=half_batch, shuffle=True, num_workers=16, pin_memory=True)\n","target_test_loader = DataLoader(target_test_dataset, batch_size=batch_size, shuffle=True, num_workers=16, pin_memory=True)\n","\n","discriminator_optim = torch.optim.Adam(discriminator.parameters())\n","target_optim = torch.optim.Adam(target_model.parameters())\n","criterion_train = nn.BCEWithLogitsLoss()\n","criterion_test = nn.NLLLoss()"]},{"cell_type":"markdown","metadata":{},"source":["### Visualize MNIST-M & MNIST Data ðŸ–¼ï¸"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["visualize_digits(dataset=target_train_dataset, k=200, mnistm=True, title='Sample MNIST-M Images')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["visualize_digits(dataset=source_dataset, k=120, cmap='gray', title='Sample MNIST Images')"]},{"cell_type":"markdown","metadata":{},"source":["### Adversarial Discriminative Domain Adaptation"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["disc_losses, disc_accuracies, disc_train_counter = [], [], []\n","clf_disc_losses, clf_disc_train_counter = [], []\n","clf_losses, clf_accuracies = [], []\n","clf_test_counter = [idx*iterations*k_clf*target_train_loader.batch_size for idx in range(0, epochs+1)]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_loss, test_accuracy = 0, 0\n","clf.eval()\n","tqdm_bar = tqdm(target_test_loader, desc=f'Testing ', total=int(len(target_test_loader)))\n","for idx, (images, labels) in enumerate(tqdm_bar):\n","    images, labels = images.to(device), labels.to(device)\n","    with torch.no_grad():\n","        outputs = clf(images)\n","        loss = criterion_test(outputs, labels)\n","    test_loss += loss.item()\n","    outputs = torch.argmax(outputs, dim=1).type(torch.FloatTensor).to(device)\n","    test_accuracy += torch.mean((outputs == labels).type(torch.FloatTensor)).item()\n","    tqdm_bar.set_postfix(test_loss=(test_loss/(idx+1)), test_accuracy=test_accuracy/(idx+1))\n","clf_losses.append(test_loss/len(target_test_loader))\n","clf_accuracies.append(test_accuracy/len(target_test_loader))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for epoch in range(epochs):\n","    target_batch_iterator = loop_iterable(target_train_loader)\n","    batch_iterator = zip(loop_iterable(source_loader), loop_iterable(target_train_loader))\n","    disc_loss, disc_accuracy = 0, 0\n","    clf_disc_loss = 0\n","    test_loss, test_accuracy = 0, 0\n","    tqdm_bar = tqdm(range(iterations), desc=f'Training Epoch {epoch} ', total=iterations)\n","    for iter_idx in tqdm_bar:\n","        # Train discriminator\n","        set_requires_grad(target_model, requires_grad=False)\n","        set_requires_grad(discriminator, requires_grad=True)\n","        for disc_idx in range(k_disc):\n","            (source_x, _), (target_x, _) = next(batch_iterator)\n","            source_x, target_x = source_x.to(device), target_x.to(device)\n","            source_features = source_model(source_x).view(source_x.shape[0], -1)\n","            target_features = target_model(target_x).view(target_x.shape[0], -1)\n","            discriminator_x = torch.cat([source_features, target_features])\n","            discriminator_y = torch.cat([torch.ones(source_x.shape[0], device=device), torch.zeros(target_x.shape[0], device=device)])\n","            preds = discriminator(discriminator_x).squeeze()\n","            loss = criterion_train(preds, discriminator_y)\n","            discriminator_optim.zero_grad()\n","            loss.backward()\n","            discriminator_optim.step()\n","            disc_loss += loss.item()\n","            disc_losses.append(loss.item())\n","            disc_batch_accuracy = ((preds > 0).long() == discriminator_y.long()).float().mean().item()\n","            disc_accuracy += disc_batch_accuracy\n","            disc_accuracies.append(disc_batch_accuracy)\n","            disc_train_counter.append((disc_idx+1)*source_x.size(0) + iter_idx*k_disc*target_train_loader.batch_size + epoch*iterations*k_disc*target_train_loader.batch_size)\n","\n","        # Train classifier\n","        set_requires_grad(target_model, requires_grad=True)\n","        set_requires_grad(discriminator, requires_grad=False)\n","        for clf_idx in range(k_clf):\n","            _, (target_x, _) = next(batch_iterator)\n","            target_x = target_x.to(device)\n","            target_features = target_model(target_x).view(target_x.shape[0], -1)\n","            # Flipped Labels\n","            discriminator_y = torch.ones(target_x.shape[0], device=device)\n","            preds = discriminator(target_features).squeeze()\n","            loss = criterion_train(preds, discriminator_y)\n","            target_optim.zero_grad()\n","            loss.backward()\n","            target_optim.step()\n","            clf_disc_loss += loss.item()\n","            clf_disc_losses.append(loss.item())\n","            clf_disc_train_counter.append(source_x.size(0) + clf_idx*half_batch + iter_idx*k_clf*half_batch + epoch*iterations*k_clf*half_batch)\n","        tqdm_bar.set_postfix(disc_loss=disc_loss/((iter_idx+1)*k_disc), disc_accuracy=disc_accuracy/((iter_idx+1)*k_disc),\n","                             clf_disc_loss=clf_disc_loss/((iter_idx+1)*k_clf))\n","\n","    # Test full target model\n","    test_loss, test_accuracy = 0, 0\n","    clf.feature_extractor = target_model\n","    clf.eval()\n","    tqdm_bar = tqdm(target_test_loader, desc=f'Testing Epoch {epoch} (Full Target Model)', total=int(len(target_test_loader)))\n","    for idx, (images, labels) in enumerate(tqdm_bar):\n","        images, labels = images.to(device), labels.to(device)\n","        with torch.no_grad():\n","            outputs = clf(images)\n","            loss = criterion_test(outputs, labels)\n","        test_loss += loss.item()\n","        outputs = torch.argmax(outputs, dim=1).type(torch.FloatTensor).to(device)\n","        test_accuracy += torch.mean((outputs == labels).type(torch.FloatTensor)).item()\n","        tqdm_bar.set_postfix(test_loss=(test_loss/(idx+1)), test_accuracy=test_accuracy/(idx+1))\n","    clf_losses.append(test_loss/len(target_test_loader))\n","    clf_accuracies.append(test_accuracy/len(target_test_loader))\n","    if np.argmax(clf_accuracies) == len(clf_accuracies)-1:\n","        torch.save(clf.state_dict(), 'adda_target_weights.pth')\n","        "]},{"cell_type":"markdown","metadata":{},"source":["#### Visualize Training & Testing Results ðŸ“ˆ"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fig = plotly.subplots.make_subplots(specs=[[{\"secondary_y\": True}]])\n","fig.add_trace(go.Scatter(x=disc_train_counter, y=disc_losses, mode='lines', name='Disc Loss'), secondary_y=False)\n","fig.add_trace(go.Scatter(x=disc_train_counter, y=disc_accuracies, mode='lines', name='Disc Accuracy', line_color='lightseagreen'), secondary_y=True)\n","fig.update_layout(\n","    width=1000,\n","    height=500,\n","    title=\"Discriminator Loss vs Accuracy\")\n","fig.update_xaxes(title_text=\"Number of training examples seen\")\n","fig.update_yaxes(title_text=\"Discriminator <b>Loss</b> (BCE)\", secondary_y=False)\n","fig.update_yaxes(title_text=\"Discriminator <b>Accuracy</b>\", secondary_y=True)\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fig = go.Figure()\n","fig.add_trace(go.Scatter(x=clf_disc_train_counter, y=clf_disc_losses, mode='lines', name='Clf-Disc Train Loss'))\n","fig.update_layout(\n","    width=1000,\n","    height=500,\n","    title=\"Clf-Disc Loss\",\n","    xaxis_title=\"Number of training examples seen\",\n","    yaxis_title=\"Binary Cross Entropy Loss\"),\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fig = plotly.subplots.make_subplots(specs=[[{\"secondary_y\": True}]])\n","fig.add_trace(go.Scatter(x=clf_test_counter, y=clf_accuracies, marker_symbol='star-diamond', \n","                         marker_line_color=\"orange\", marker_line_width=1, marker_size=9, mode='lines+markers', \n","                         name='Target Accuracy'), secondary_y=False)\n","fig.add_trace(go.Scatter(x=clf_test_counter, y=clf_losses, marker_symbol='star-square', \n","                         marker_line_color=\"lightseagreen\", marker_line_width=1, marker_size=9, mode='lines+markers',\n","                         name='Target Loss'), secondary_y=True)\n","fig.update_layout(\n","    width=1000,\n","    height=500,\n","    title=\"Full Target Model Loss vs Accuracy\")\n","fig.update_xaxes(title_text=\"Number of training examples seen\")\n","fig.update_yaxes(title_text=\"Target <b>Accuracy</b>\", secondary_y=False)\n","fig.update_yaxes(title_text=\"Target <b>Loss</b> (NLLLoss)\", secondary_y=True)\n","fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["## How to improve the model?\n","\n","Try the following:\n","- Modify the base Model (see `Net` class), try a deeper network.\n","- Change Hyperparameters: see `k_disc`, `k_clf`.\n","- Modify the discriminator architecture\n","- Add Data agumentation to improve generalization. Check:\n","    - MixUp: https://pytorch.org/vision/main/generated/torchvision.transforms.v2.MixUp.html\n","    - CutMix: https://pytorch.org/vision/main/generated/torchvision.transforms.v2.CutMix.html#torchvision.transforms.v2.CutMix\n","    - How to: https://pytorch.org/vision/main/auto_examples/transforms/plot_cutmix_mixup.html#sphx-glr-auto-examples-transforms-plot-cutmix-mixup-py\n","    - More augmentations: https://pytorch.org/vision/main/auto_examples/transforms/index.html"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30017,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}
