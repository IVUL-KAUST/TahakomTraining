{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30017,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction\n\nAdversarial Discriminative Domain Adaptation (ADDA)\n\nIn real-world applications, machine learning models are often trained on a dataset that is not perfectly representative of the domain where the model will be deployed. This difference between the source domain (where the model is trained) and the target domain (where the model is applied) is called **domain shift**, and it often leads to a significant drop in performance when models trained on the source domain are tested on the target domain.\n\n<div style=\"text-align:center\">\n<img src=\"https://i.ibb.co/v3x80W9/Screenshot-2024-10-22-at-11-24-32-AM.png\" alt=\"Adversarial Discriminative Domain Adaptation\" border=\"0\" width=400>\n</div>\n\nAdversarial Discriminative Domain Adaptation (ADDA) is a technique designed to address this problem through unsupervised domain adaptation. The key idea is to transfer the knowledge learned from a labeled source domain to an unlabeled target domain by aligning the feature distributions between the two domains, See above Figure. ADDA achieves this through a two-step process:\n\n1. Feature learning on the source domain: A feature extractor is trained on the labeled source domain in a supervised manner.\n2. Adversarial adaptation for the target domain: The model then uses adversarial training to align the target domain features with those from the source domain, ensuring that the feature representations are domain-invariant.\n\nThe method is inspired by the concept of Generative Adversarial Networks (GANs), where a domain discriminator attempts to distinguish between source and target domain features, while the target feature extractor learns to fool the discriminator by producing features that are indistinguishable between domains.\n\nObjectives:\n\n- Learn domain-invariant features: Align the feature spaces of the source and target domains without requiring labels from the target domain.\n- Improve generalization: Train a model that performs well on the target domain despite the domain shift.\n\nThis notebook will guide you through the implementation of ADDA, covering key steps such as:\n\n- Pretraining a feature extractor on the source domain.\n- Applying adversarial training for domain adaptation.\n- Evaluating the performance on the target domain.\n\nBy the end of this notebook, you will understand how adversarial techniques can be used to adapt a model to a new, unlabeled domain.\n","metadata":{}},{"cell_type":"markdown","source":"## Model Overview\n\n<div style=\"text-align:center\">\n    <img src=\"https://i.ibb.co/2sCRxmF/Screenshot-2024-10-22-at-11-29-59-AM.png\" alt=\"Screenshot-2024-10-22-at-11-29-59-AM\" border=\"0\" width=800>\n</div>\n\nPaper: https://arxiv.org/pdf/1702.05464\n\n#### NOTE:\n**Here we have MNIST as Source images and MNIST-M as Target**","metadata":{}},{"cell_type":"markdown","source":"## Pretraining a feature extractor on the source domain (MNIST)","metadata":{}},{"cell_type":"markdown","source":"### Libraries ðŸ“šâ¬‡","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom pathlib import Path\nimport random, math, cv2\nfrom tqdm import tqdm_notebook as tqdm\n\nimport torch\nfrom torch import nn\nfrom torchvision.datasets import MNIST\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import datasets, transforms\nfrom torchvision.transforms import Compose, ToTensor\n\nimport matplotlib.pyplot as plt\nimport plotly\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize_digits(dataset, k=80, mnistm=False, cmap=None, title=None):\n    \n    ncols = 20\n    indices = random.choices(range(len(dataset)), k=k)\n    nrows = math.floor(len(indices)/ncols)\n    \n    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols,nrows+0.4), gridspec_kw=dict(wspace=0.1, hspace=0.1), subplot_kw=dict(yticks=[], xticks=[]))\n    axes_flat = axes.reshape(-1)\n    fig.suptitle(title, fontsize=20)\n    \n    for list_idx, image_idx in enumerate(indices[:ncols*nrows]):\n        ax = axes_flat[list_idx]\n        image = dataset[image_idx][0]\n        image = image.numpy().transpose(1, 2, 0)\n        ax.imshow(image, cmap=cmap)\n\ndef set_requires_grad(model, requires_grad=True):\n    for param in model.parameters():\n        param.requires_grad = requires_grad\n\ndef loop_iterable(iterable):\n    while True:\n        yield from iterable\n\nclass GrayscaleToRgb:\n    \"\"\"Convert a grayscale image to rgb\"\"\"\n    def __call__(self, image):\n        image = np.array(image)\n        image = np.dstack([image, image, image])\n        return Image.fromarray(image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MNIST_DATA_DIR = Path('/kaggle/working')\nBSDS_DATA_DIR = Path('/kaggle/working/bsds500')\nMODEL_FILE = Path('best_source_weights_mnist.pth')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Hyperparameters\nbatch_size = 64\nepochs = 5\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Download the data","metadata":{}},{"cell_type":"code","source":"!wget https://raw.githubusercontent.com/fgnt/mnist/master/train-images-idx3-ubyte.gz\n!wget https://raw.githubusercontent.com/fgnt/mnist/master/train-labels-idx1-ubyte.gz\n!wget https://raw.githubusercontent.com/fgnt/mnist/master/t10k-images-idx3-ubyte.gz\n!wget https://raw.githubusercontent.com/fgnt/mnist/master/t10k-labels-idx1-ubyte.gz\n!mv train-images-idx3-ubyte.gz /kaggle/working/mnist/MNIST/raw/\n!mv train-labels-idx1-ubyte.gz /kaggle/working/mnist/MNIST/raw/\n!mv t10k-images-idx3-ubyte.gz /kaggle/working/mnist/MNIST/raw/\n!mv t10k-labels-idx1-ubyte.gz /kaggle/working/mnist/MNIST/raw/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!curl -L -o bsds500.zip https://www.kaggle.com/api/v1/datasets/download/balraj98/berkeley-segmentation-dataset-500-bsds500\n!mkdir bsds500\n!unzip -q bsds500.zip -d bsds500/\n!rm bsds500.zip","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define Model","metadata":{}},{"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.feature_extractor = nn.Sequential(\n            nn.Conv2d(3, 10, kernel_size=5),\n            nn.MaxPool2d(2),\n            nn.ReLU(),\n            nn.Conv2d(10, 20, kernel_size=5),\n            nn.MaxPool2d(2),\n            nn.Dropout2d(),\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Linear(320, 50),\n            nn.ReLU(),\n            nn.Dropout(),\n            nn.Linear(50, 10),\n            nn.LogSoftmax(),\n        )\n\n    def forward(self, x):\n        features = self.feature_extractor(x)\n        features = features.view(x.shape[0], -1)\n        logits = self.classifier(features)\n        return logits","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Get Dataset & Dataloaders","metadata":{}},{"cell_type":"code","source":"source_model = Net().to(device)\nif MODEL_FILE.exists():\n    source_model.load_state_dict(torch.load(MODEL_FILE))\n\ntrain_dataset = MNIST(MNIST_DATA_DIR / 'mnist', train=True, download=True, transform=Compose([GrayscaleToRgb(), ToTensor()]))\ntest_dataset = MNIST(MNIST_DATA_DIR / 'mnist', train=False, download=True, transform=Compose([GrayscaleToRgb(), ToTensor()]))\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=32, pin_memory=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=32, pin_memory=True)\n\nsource_optim = torch.optim.Adam(source_model.parameters(), lr=0.002)\ncriterion = nn.NLLLoss()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualize MNIST Data","metadata":{}},{"cell_type":"code","source":"visualize_digits(dataset=train_dataset, k=120, cmap='gray', title='Sample MNIST Images')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train Source Model on MNIST","metadata":{}},{"cell_type":"code","source":"train_losses, train_accuracies, train_counter = [], [], []\ntest_losses, test_accuracies = [], []\ntest_counter = [idx*len(train_loader.dataset) for idx in range(0, epochs+1)]\n\ndef train(epoch):\n    train_loss, train_accuracy = 0, 0\n    source_model.train()\n    tqdm_bar = tqdm(train_loader, desc=f'Training Epoch {epoch} ', total=int(len(train_loader)))\n    for idx, (images, labels) in enumerate(tqdm_bar):\n        images, labels = images.to(device), labels.to(device)\n        source_optim.zero_grad()\n        with torch.set_grad_enabled(True):\n            outputs = source_model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            source_optim.step()\n        train_loss += loss.item()\n        train_losses.append(loss.item())\n        outputs = torch.argmax(outputs, dim=1).type(torch.FloatTensor).to(device)\n        train_batch_accuracy = torch.mean((outputs == labels).type(torch.FloatTensor)).item()\n        train_accuracy += train_batch_accuracy\n        train_accuracies.append(train_batch_accuracy)\n        tqdm_bar.set_postfix(train_loss=(train_loss/(idx+1)), train_accuracy=train_accuracy/(idx+1))\n        train_counter.append(idx*batch_size + images.size(0) + epoch*len(train_dataset))\n\ndef test():\n    test_loss, test_accuracy = 0, 0\n    source_model.eval()\n    tqdm_bar = tqdm(test_loader, desc=f'Testing ', total=int(len(test_loader)))\n    for idx, (images, labels) in enumerate(tqdm_bar):\n        images, labels = images.to(device), labels.to(device)\n        with torch.no_grad():\n            outputs = source_model(images)\n            loss = criterion(outputs, labels)\n        test_loss += loss.item()\n        outputs = torch.argmax(outputs, dim=1).type(torch.FloatTensor).to(device)\n        test_accuracy += torch.mean((outputs == labels).type(torch.FloatTensor)).item()\n        tqdm_bar.set_postfix(test_loss=(test_loss/(idx+1)), test_accuracy=test_accuracy/(idx+1))\n    test_losses.append(test_loss/len(test_loader))\n    test_accuracies.append(test_accuracy/len(test_loader))\n    if np.argmax(test_accuracies) == len(test_accuracies)-1:\n        torch.save(source_model.state_dict(), 'best_source_weights_mnist.pth')\n        \ntest()\nfor epoch in range(epochs):\n    train(epoch)\n    test()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Visualize Training & Testing Results ðŸ“ˆ","metadata":{}},{"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=train_counter, y=train_losses, mode='lines', name='Train loss'))\nfig.add_trace(go.Scatter(x=test_counter, y=test_losses, marker_symbol='star-diamond', \n                         marker_color='orange', marker_line_width=1, marker_size=9, mode='markers', name='Test loss'))\nfig.update_layout(\n    width=1000,\n    height=500,\n    title=\"Train vs. Test Loss\",\n    xaxis_title=\"Number of training examples seen\",\n    yaxis_title=\"Negative Log Likelihood loss\"),\nfig.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=train_counter, y=train_accuracies, mode='lines', name='Train loss'))\nfig.add_trace(go.Scatter(x=test_counter, y=test_accuracies, marker_symbol='star-diamond', \n                         marker_color='orange', marker_line_width=1, marker_size=9, mode='markers', name='Test Accuracy'))\nfig.update_layout(\n    width=1000,\n    height=500,\n    title=\"Train vs. Test Accuracy\",\n    xaxis_title=\"Number of training examples seen\",\n    yaxis_title=\"Accuracy\")\nfig.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Adversarial training for domain adaptation","metadata":{}},{"cell_type":"markdown","source":"Now we will load the pre-trained model on source domain and apply the adversarial training for domain adaptation","metadata":{}},{"cell_type":"code","source":"# Hyperparameters\n\nbatch_size = 64\niterations = 500\nepochs = 4\nk_disc = 1\nk_clf = 10\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`MODEL_FILE` contains the path to pretrained source model","metadata":{}},{"cell_type":"code","source":"source_model = Net().to(device)\nif MODEL_FILE:\n    source_model.load_state_dict(torch.load(MODEL_FILE, map_location=device))\nsource_model.eval()\nset_requires_grad(source_model, requires_grad=False)\n\nclf = source_model\nsource_model = source_model.feature_extractor\n\ntarget_model = Net().to(device)\nif MODEL_FILE:\n    target_model.load_state_dict(torch.load(MODEL_FILE, map_location=device))\ntarget_model = target_model.feature_extractor\n\ndiscriminator = nn.Sequential(\n    nn.Linear(320, 120),\n    nn.ReLU(),\n    nn.Linear(120, 20),\n    nn.ReLU(),\n    nn.Linear(20, 1)\n).to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define Dataset Classes","metadata":{}},{"cell_type":"code","source":"class BSDS500(Dataset):\n\n    def __init__(self):\n        image_folder = BSDS_DATA_DIR / 'images'\n        self.image_files = list(map(str, image_folder.glob('*/*.jpg')))\n\n    def __getitem__(self, i):\n        image = cv2.imread(self.image_files[i], cv2.IMREAD_COLOR)\n        tensor = torch.from_numpy(image.transpose(2, 0, 1))\n        return tensor\n\n    def __len__(self):\n        return len(self.image_files)\n\n\nclass MNISTM(Dataset):\n\n    def __init__(self, train=True):\n        super(MNISTM, self).__init__()\n        self.mnist = datasets.MNIST(MNIST_DATA_DIR / 'mnist', train=train,\n                                    download=True)\n        self.bsds = BSDS500()\n        # Fix RNG so the same images are used for blending\n        self.rng = np.random.RandomState(42)\n\n    def __getitem__(self, i):\n        digit, label = self.mnist[i]\n        digit = transforms.ToTensor()(digit)\n        bsds_image = self._random_bsds_image()\n        patch = self._random_patch(bsds_image)\n        patch = patch.float() / 255\n        blend = torch.abs(patch - digit)\n        return blend, label\n\n    def _random_patch(self, image, size=(28, 28)):\n        _, im_height, im_width = image.shape\n        x = self.rng.randint(0, im_width-size[1])\n        y = self.rng.randint(0, im_height-size[0])\n        return image[:, y:y+size[0], x:x+size[1]]\n\n    def _random_bsds_image(self):\n        i = self.rng.choice(len(self.bsds))\n        return self.bsds[i]\n\n    def __len__(self):\n        return len(self.mnist)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Get Dataset & Dataloaders","metadata":{}},{"cell_type":"code","source":"half_batch = batch_size // 2\n\nsource_dataset = MNIST(MNIST_DATA_DIR/'mnist', train=True, download=True, transform=Compose([GrayscaleToRgb(), ToTensor()]))\nsource_loader = DataLoader(source_dataset, batch_size=half_batch, shuffle=True, num_workers=16, pin_memory=True)\n\ntarget_train_dataset, target_test_dataset = MNISTM(train=True), MNISTM(train=False)\ntarget_train_loader = DataLoader(target_train_dataset, batch_size=half_batch, shuffle=True, num_workers=16, pin_memory=True)\ntarget_test_loader = DataLoader(target_test_dataset, batch_size=batch_size, shuffle=True, num_workers=16, pin_memory=True)\n\ndiscriminator_optim = torch.optim.Adam(discriminator.parameters())\ntarget_optim = torch.optim.Adam(target_model.parameters())\ncriterion_train = nn.BCEWithLogitsLoss()\ncriterion_test = nn.NLLLoss()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualize MNIST-M & MNIST Data ðŸ–¼ï¸","metadata":{}},{"cell_type":"code","source":"visualize_digits(dataset=target_train_dataset, k=200, mnistm=True, title='Sample MNIST-M Images')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_digits(dataset=source_dataset, k=120, cmap='gray', title='Sample MNIST Images')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Adversarial Discriminative Domain Adaptation","metadata":{}},{"cell_type":"code","source":"disc_losses, disc_accuracies, disc_train_counter = [], [], []\nclf_disc_losses, clf_disc_train_counter = [], []\nclf_losses, clf_accuracies = [], []\nclf_test_counter = [idx*iterations*k_clf*target_train_loader.batch_size for idx in range(0, epochs+1)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loss, test_accuracy = 0, 0\nclf.eval()\ntqdm_bar = tqdm(target_test_loader, desc=f'Testing ', total=int(len(target_test_loader)))\nfor idx, (images, labels) in enumerate(tqdm_bar):\n    images, labels = images.to(device), labels.to(device)\n    with torch.no_grad():\n        outputs = clf(images)\n        loss = criterion_test(outputs, labels)\n    test_loss += loss.item()\n    outputs = torch.argmax(outputs, dim=1).type(torch.FloatTensor).to(device)\n    test_accuracy += torch.mean((outputs == labels).type(torch.FloatTensor)).item()\n    tqdm_bar.set_postfix(test_loss=(test_loss/(idx+1)), test_accuracy=test_accuracy/(idx+1))\nclf_losses.append(test_loss/len(target_test_loader))\nclf_accuracies.append(test_accuracy/len(target_test_loader))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(epochs):\n    target_batch_iterator = loop_iterable(target_train_loader)\n    batch_iterator = zip(loop_iterable(source_loader), loop_iterable(target_train_loader))\n    disc_loss, disc_accuracy = 0, 0\n    clf_disc_loss = 0\n    test_loss, test_accuracy = 0, 0\n    tqdm_bar = tqdm(range(iterations), desc=f'Training Epoch {epoch} ', total=iterations)\n    for iter_idx in tqdm_bar:\n        # Train discriminator\n        set_requires_grad(target_model, requires_grad=False)\n        set_requires_grad(discriminator, requires_grad=True)\n        for disc_idx in range(k_disc):\n            (source_x, _), (target_x, _) = next(batch_iterator)\n            source_x, target_x = source_x.to(device), target_x.to(device)\n            source_features = source_model(source_x).view(source_x.shape[0], -1)\n            target_features = target_model(target_x).view(target_x.shape[0], -1)\n            discriminator_x = torch.cat([source_features, target_features])\n            discriminator_y = torch.cat([torch.ones(source_x.shape[0], device=device), torch.zeros(target_x.shape[0], device=device)])\n            preds = discriminator(discriminator_x).squeeze()\n            loss = criterion_train(preds, discriminator_y)\n            discriminator_optim.zero_grad()\n            loss.backward()\n            discriminator_optim.step()\n            disc_loss += loss.item()\n            disc_losses.append(loss.item())\n            disc_batch_accuracy = ((preds > 0).long() == discriminator_y.long()).float().mean().item()\n            disc_accuracy += disc_batch_accuracy\n            disc_accuracies.append(disc_batch_accuracy)\n            disc_train_counter.append((disc_idx+1)*source_x.size(0) + iter_idx*k_disc*target_train_loader.batch_size + epoch*iterations*k_disc*target_train_loader.batch_size)\n\n        # Train classifier\n        set_requires_grad(target_model, requires_grad=True)\n        set_requires_grad(discriminator, requires_grad=False)\n        for clf_idx in range(k_clf):\n            _, (target_x, _) = next(batch_iterator)\n            target_x = target_x.to(device)\n            target_features = target_model(target_x).view(target_x.shape[0], -1)\n            # Flipped Labels\n            discriminator_y = torch.ones(target_x.shape[0], device=device)\n            preds = discriminator(target_features).squeeze()\n            loss = criterion_train(preds, discriminator_y)\n            target_optim.zero_grad()\n            loss.backward()\n            target_optim.step()\n            clf_disc_loss += loss.item()\n            clf_disc_losses.append(loss.item())\n            clf_disc_train_counter.append(source_x.size(0) + clf_idx*half_batch + iter_idx*k_clf*half_batch + epoch*iterations*k_clf*half_batch)\n        tqdm_bar.set_postfix(disc_loss=disc_loss/((iter_idx+1)*k_disc), disc_accuracy=disc_accuracy/((iter_idx+1)*k_disc),\n                             clf_disc_loss=clf_disc_loss/((iter_idx+1)*k_clf))\n\n    # Test full target model\n    test_loss, test_accuracy = 0, 0\n    clf.feature_extractor = target_model\n    clf.eval()\n    tqdm_bar = tqdm(target_test_loader, desc=f'Testing Epoch {epoch} (Full Target Model)', total=int(len(target_test_loader)))\n    for idx, (images, labels) in enumerate(tqdm_bar):\n        images, labels = images.to(device), labels.to(device)\n        with torch.no_grad():\n            outputs = clf(images)\n            loss = criterion_test(outputs, labels)\n        test_loss += loss.item()\n        outputs = torch.argmax(outputs, dim=1).type(torch.FloatTensor).to(device)\n        test_accuracy += torch.mean((outputs == labels).type(torch.FloatTensor)).item()\n        tqdm_bar.set_postfix(test_loss=(test_loss/(idx+1)), test_accuracy=test_accuracy/(idx+1))\n    clf_losses.append(test_loss/len(target_test_loader))\n    clf_accuracies.append(test_accuracy/len(target_test_loader))\n    if np.argmax(clf_accuracies) == len(clf_accuracies)-1:\n        torch.save(clf.state_dict(), 'adda_target_weights.pth')\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Visualize Training & Testing Results ðŸ“ˆ","metadata":{}},{"cell_type":"code","source":"fig = plotly.subplots.make_subplots(specs=[[{\"secondary_y\": True}]])\nfig.add_trace(go.Scatter(x=disc_train_counter, y=disc_losses, mode='lines', name='Disc Loss'), secondary_y=False)\nfig.add_trace(go.Scatter(x=disc_train_counter, y=disc_accuracies, mode='lines', name='Disc Accuracy', line_color='lightseagreen'), secondary_y=True)\nfig.update_layout(\n    width=1000,\n    height=500,\n    title=\"Discriminator Loss vs Accuracy\")\nfig.update_xaxes(title_text=\"Number of training examples seen\")\nfig.update_yaxes(title_text=\"Discriminator <b>Loss</b> (BCE)\", secondary_y=False)\nfig.update_yaxes(title_text=\"Discriminator <b>Accuracy</b>\", secondary_y=True)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=clf_disc_train_counter, y=clf_disc_losses, mode='lines', name='Clf-Disc Train Loss'))\nfig.update_layout(\n    width=1000,\n    height=500,\n    title=\"Clf-Disc Loss\",\n    xaxis_title=\"Number of training examples seen\",\n    yaxis_title=\"Binary Cross Entropy Loss\"),\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plotly.subplots.make_subplots(specs=[[{\"secondary_y\": True}]])\nfig.add_trace(go.Scatter(x=clf_test_counter, y=clf_accuracies, marker_symbol='star-diamond', \n                         marker_line_color=\"orange\", marker_line_width=1, marker_size=9, mode='lines+markers', \n                         name='Target Accuracy'), secondary_y=False)\nfig.add_trace(go.Scatter(x=clf_test_counter, y=clf_losses, marker_symbol='star-square', \n                         marker_line_color=\"lightseagreen\", marker_line_width=1, marker_size=9, mode='lines+markers',\n                         name='Target Loss'), secondary_y=True)\nfig.update_layout(\n    width=1000,\n    height=500,\n    title=\"Full Target Model Loss vs Accuracy\")\nfig.update_xaxes(title_text=\"Number of training examples seen\")\nfig.update_yaxes(title_text=\"Target <b>Accuracy</b>\", secondary_y=False)\nfig.update_yaxes(title_text=\"Target <b>Loss</b> (NLLLoss)\", secondary_y=True)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## How to improve the model?\n\nTry the following:\n- Modify the base Model (see `Net` class), try a deeper network.\n- Change Hyperparameters: see `k_disc`, `k_clf`.\n- Modify the discriminator architecture\n- Add Data agumentation to improve generalization. Check:\n    - MixUp: https://pytorch.org/vision/main/generated/torchvision.transforms.v2.MixUp.html\n    - CutMix: https://pytorch.org/vision/main/generated/torchvision.transforms.v2.CutMix.html#torchvision.transforms.v2.CutMix\n    - How to: https://pytorch.org/vision/main/auto_examples/transforms/plot_cutmix_mixup.html#sphx-glr-auto-examples-transforms-plot-cutmix-mixup-py\n    - More augmentations: https://pytorch.org/vision/main/auto_examples/transforms/index.html","metadata":{}}]}