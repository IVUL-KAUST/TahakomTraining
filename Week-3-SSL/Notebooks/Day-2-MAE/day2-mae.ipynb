{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"markdown","metadata":{},"source":["Install some dependencies:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install einops"]},{"cell_type":"markdown","metadata":{},"source":["# Masked Autoencoder\n","\n","Implementation of Masked Autoencoders Are Scalable Vision Learners paper on CIFAR datasets.\n","\n","**Implementation details:** Due to limit resource available, we only test the model on cifar10. We mainly want to reproduce the result that pre-training an ViT with MAE can achieve a better result than directly trained in supervised learning with labels. This should be an evidence of self-supervised learning is more data efficient than supervised learning.\n","\n","We mainly follow the implementation details in the paper. However, due to difference between Cifar10 and ImageNet, we make some modification:\n","\n","- We use vit-tiny instead of vit-base.\n","- Since Cifar10 have only 50k training data, we increase the pretraining epoch from 400 to 2000, and the warmup epoch from 40 to 200. We noticed that, the loss is still decreasing after 2000 epoches.\n","- We decrease the batch size for training the classifier from 1024 to 128 to mitigate the overfitting."]},{"cell_type":"markdown","metadata":{},"source":["## Learn to Patchify"]},{"cell_type":"markdown","metadata":{},"source":["### Load Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","from torchvision.datasets import CIFAR10\n","from torch.utils.data import DataLoader\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Constants\n","BUFFER_SIZE = 1024\n","BATCH_SIZE = 256\n","IMAGE_SIZE = 48\n","PATCH_SIZE = 6\n","NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n","MASK_PROPORTION = 0.75\n","EPOCHS = 250\n","DOWNSTREAM_EPOCHS = 250\n","\n","# Data transformations\n","train_transform = transforms.Compose([\n","#     transforms.RandomResizedCrop(IMAGE_SIZE),\n","#     transforms.RandomHorizontalFlip(),\n","    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n","    transforms.ToTensor(),\n","])\n","\n","test_transform = transforms.Compose([\n","    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n","    transforms.ToTensor(),\n","])\n","\n","# CIFAR10 dataset\n","train_dataset = CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n","test_dataset = CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"]},{"cell_type":"markdown","metadata":{},"source":["### Extracting Patches\n","\n","![Transformer Architecture](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W9ecVUyrjTn2RY9Ufuj88Q.png)\n","\n","![Patchify](https://miro.medium.com/v2/resize:fit:1160/format:webp/1*f0lDgBvf-nc4IytWmJAEbw.png)\n","\n","We will focus on the first step of the Vision Transformer, which involves converting images into patches. To achieve this, we will utilize PyTorch’s built-in technique called ‘Unfold.’ We will also explain how Unfold works and demonstrate its application in the context of “patchifying” an image.\n","\n","The Unfold function in PyTorch enables access to specific parts of a tensor, allowing for further processing. It extracts blocks from a tensor in a sliding manner. It as similar to the max-pooling or average-pooling operation, where a specified block size is processed, and then the operation slides to the next block. Unfold provides the ability to extract values from these sliding blocks. Additionally, the Unfold operation flattens the values within each block. Let’s illustrate this with an example.\n","\n","![Unfold Illustration](https://miro.medium.com/v2/resize:fit:1018/format:webp/1*yUojSXo2Qrs18jhg-lTNTw.png)\n","\n","\n","For simplicity, let’s consider a tensor with a shape of [1 x 1 x 3 x 3], where the first dimension is the batch size, the second is the number of channels, and the last two are the height and width of the tensor. If we define the unfold operation with a kernel size of (2,2), a block of this size will slide through the tensor, extracting values, flattening them, and moving to the next position. Initially, it will extract values [1, 2, 4, 5], then slide to [2, 3, 5, 6], and continue this process by sliding down. The resultant tensor would have a shape of [1 x 4 x 4], where 1 is the batch size, 4 represents (patch size * number of channels), and the last dimension indicates the number of blocks it has slid through. The exact formula for the output tensor is provided in the documentation (which I will link at the end). The unfold operation in PyTorch is defined as:\n","\n","```\n","torch.nn.Unfold(kernel_size, dilation=1, padding=0, stride=1)\n","```\n","\n","The parameters are similar to what we set in pooling/conv operations in PyTorch."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","class Patches(nn.Module):\n","    def __init__(self, patch_size):\n","        super(Patches, self).__init__()\n","        self.patch_size = patch_size\n","        # TODO: Define the Unfold object. Kernel_size and stride should be defined.\n","        self.unfold = ...\n","\n","    def forward(self, images):\n","        # images -> B c h w\n","        bs, c, h, w = images.shape\n","        \n","        images = ... # TODO: use unfold with the images\n","        # images -> B (c*p*p) L\n","        \n","        # Reshaping into the shape we want\n","        patches = images.view(bs, c, self.patch_size, self.patch_size, -1).permute(0, 4, 1, 2, 3)\n","        # patches -> ( B no.of patches c p p )\n","        return patches\n","\n","    def show_patched_image(self, images, patches):\n","        idx = np.random.choice(patches.shape[0])  # Randomly select an image from the batch\n","        print(f\"Index selected: {idx}.\")\n","\n","        # Show the original image\n","        plt.figure(figsize=(4, 4))\n","        original_image = images[idx].permute(1, 2, 0).cpu().numpy()  # Convert to (H, W, C) format\n","        plt.imshow(original_image)\n","        plt.suptitle('Original Image')\n","        plt.axis(\"off\")\n","        plt.show()\n","\n","        # Display patches\n","        n_p = int(np.sqrt(patches.shape[1]))  # Number of patches along one side\n","        plt.figure(figsize=(4, 4))\n","        plt.suptitle('Patches')\n","        for i, patch in enumerate(patches[idx]):\n","            ax = plt.subplot(n_p, n_p, i + 1)\n","            # Permute to move channels to the last dimension (from [3, 6, 6] to [6, 6, 3])\n","            patch_img = patch.permute(1, 2, 0).cpu().numpy()  # Move channels to the last dimension\n","            plt.imshow(patch_img)\n","            plt.axis(\"off\")\n","        plt.show()\n","\n","        return idx\n","\n","    def reconstruct_from_patch(self, patch):\n","        # Reconstruct the original image from patches\n","        num_patches = patch.shape[0]\n","        n_p = int(np.sqrt(num_patches))  # Assuming the patches form a square grid (e.g., 8x8 = 64 patches)\n","\n","        # Reshape patches to keep the channels dimension first and prepare for stitching the patches\n","        # The patches are currently in shape [num_patches, 3, 6, 6]\n","        # We split the patches into rows (i.e., n_p patches per row)\n","\n","        rows = torch.split(patch, n_p, dim=0)  # Split patches into rows (e.g., 8 rows of 8 patches)\n","\n","        # Now, for each row, concatenate the patches along the width (dimension 3)\n","        rows = [torch.cat(list(row), dim=2) for row in rows]  # Concatenate patches in each row along the width\n","\n","        # Finally, concatenate all the rows along the height (dimension 2)\n","        reconstructed = torch.cat(rows, dim=1)  # Concatenate rows along the height\n","\n","        # The result will have shape [3, original_height, original_width]\n","        return reconstructed\n","    \n","patch_layer = Patches(PATCH_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Grab a batch of images from the DataLoader\n","image_batch = next(iter(train_loader))\n","\n","images = image_batch[0]  # Get the images from the batch\n","\n","# Extract patches from the augmented images\n","patches = patch_layer(images)\n","print(patches.shape)\n","# Show the patched image\n","random_index = patch_layer.show_patched_image(images, patches)\n","\n","ori_images = patch_layer.reconstruct_from_patch(patches[random_index])\n","\n","# Show the original image\n","plt.figure(figsize=(4, 4))\n","original_image = ori_images.permute(1, 2, 0).cpu().numpy()  # Convert to (H, W, C) format\n","plt.imshow(original_image)\n","plt.suptitle('Reconstructed Image')\n","plt.axis(\"off\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["**Note:** In implementation, it is common to use `torch.nn.Conv2d(3, emb_dim, patch_size, patch_size)` to break the input image into patches and project them into an embedding space, i.e. patchify + linear projection (see ViT architecture).\n","\n","```\n","self.patchify = torch.nn.Conv2d(3, emb_dim, patch_size, patch_size)\n","```\n","\n","- Input Channels (3): This refers to the number of input channels, which in this case is 3, corresponding to the RGB channels of an image.\n","- Output Channels (emb_dim): This refers to the output channels (or embedding dimension). The patches from the image are projected into this embedding space. Each patch will have a feature vector of length emb_dim.\n","- Kernel Size (patch_size): The convolution’s kernel size is set to patch_size. This means the convolutional layer will “extract” patches of size patch_size × patch_size from the image.\n","- Stride (patch_size): The stride is also set to patch_size, which ensures that the patches are non-overlapping. Each patch is treated independently and projected to the embedding dimension.\n","\n","Both the `Patches` class we defined above and the Conv2d method help in extracting patches from an image. However, Conv2d not only extracts patches but also projects them into a new embedding space, which is commonly used in models where we need to represent each patch with learnable features."]},{"cell_type":"markdown","metadata":{},"source":["## Masking "]},{"cell_type":"markdown","metadata":{},"source":["Masking is a technique often used in self-supervised learning which involves hiding (or “masking”) parts of the input data (e.g., patches of an image) and then training the model to predict or reconstruct the missing parts. This helps the model learn better representations without needing explicit labels.\n","\n","There are different approach to do the masking. In this code, we focus on randomly shuffling patches of an image and masking a portion of them based on a specified ratio. The remaining patches are kept for further processing, while the others are discarded.\n","\n","Overview of the Approach:\n","\n","1. Patch Shuffling: The patches of an image are shuffled using randomly generated indexes.\n","2. Masking (Reducing Patches): A portion of the shuffled patches is removed based on the provided ratio.\n","3. Index Tracking: We keep track of both the shuffled (forward_indexes) and original (backward_indexes) positions of the patches, so the shuffling can be reversed if needed."]},{"cell_type":"markdown","metadata":{},"source":["**Hint**\n","Let’s consider a simple example with 4 patches:\n","\n","1. Original Order: [patch_0, patch_1, patch_2, patch_3]\n","2. Forward Shuffle (forward_indexes):\n"," - Suppose forward_indexes = [2, 0, 3, 1].\n"," After applying forward_indexes, the new shuffled order of the patches would be: [patch_2, patch_0, patch_3, patch_1]\n","3. Backward Indexes (backward_indexes):\n"," - To undo this shuffle, we need backward_indexes. In this case, backward_indexes = [1, 3, 0, 2].\n"," - Applying backward_indexes to the shuffled patches would restore the original order: [patch_0, patch_1, patch_2, patch_3]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def random_indexes(size : int):\n","    # TODO: Generate forward and backward indexes\n","    # should return forward_indexes, backward_indexes\n","    pass\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Check\n","\n","original_order = ['patch_0', 'patch_1', 'patch_2', 'patch_3']\n","\n","# Step 1: Generate forward and backward indexes\n","forward_indexes, backward_indexes = random_indexes(len(original_order))\n","\n","# Step 2: Apply forward_indexes to shuffle the original order\n","shuffled_order = [original_order[i] for i in forward_indexes]\n","\n","# Step 3: Apply backward_indexes to restore the original order\n","restored_order = [shuffled_order[i] for i in backward_indexes]\n","\n","# Print the results\n","print(\"Original Order:\", original_order)\n","print(\"Forward Shuffle (forward_indexes):\", forward_indexes)\n","print(\"Shuffled Order:\", shuffled_order)\n","print(\"Backward Indexes (backward_indexes):\", backward_indexes)\n","print(\"Restored Order:\", restored_order)\n","\n","# Check if the restored order matches the original\n","assert restored_order == original_order, \"Restored order does not match the original!\"\n","print(\"Check Passed: Restored order matches the original.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# This function reorders the sequences tensor along the first dimension (patches) based on the given indexes, ensuring the shape matches by expanding indexes to match the sequences tensor.\n","def take_indexes(sequences, indexes):\n","    return torch.gather(sequences, 0, repeat(indexes, 't b -> t b c', c=sequences.shape[-1]))"]},{"cell_type":"markdown","metadata":{},"source":["Example use of `take_indexes` function:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from einops import repeat, rearrange\n","from einops.layers.torch import Rearrange\n","\n","# Original tensor of patches: [T, B, C] where T=4, B=1, C=3\n","sequences = torch.tensor([\n","    [[0, 0, 0]],  # patch_0\n","    [[1, 1, 1]],  # patch_1\n","    [[2, 2, 2]],  # patch_2\n","    [[3, 3, 3]],  # patch_3\n","])  # Shape: [T=4, B=1, C=3]\n","\n","# Forward indexes to shuffle the patches\n","forward_indexes = torch.tensor([2, 0, 3, 1])  # Shape: [T=4]\n","\n","# This function reorders the sequences tensor along the first dimension (patches) based on the given indexes\n","def take_indexes(sequences, indexes):\n","    return torch.gather(sequences, 0, repeat(indexes, 't b -> t b c', c=sequences.shape[-1]))\n","\n","# Reorder patches using take_indexes\n","# Expand forward_indexes to match sequences and shuffle patches\n","shuffled_patches = take_indexes(sequences, forward_indexes.view(-1, 1))\n","\n","# Print original patches and shuffled patches\n","print(\"Original Patches:\\n\", sequences)\n","print(\"\\nShuffled Patches:\\n\", shuffled_patches)"]},{"cell_type":"markdown","metadata":{},"source":["Explanation:\n","\n","- repeat(indexes, 't b -> t b c', c=sequences.shape[-1]): This line reshapes and repeats the indexes tensor to match the size of the sequences tensor for the batch and channel dimensions (B and C).\n","- torch.gather: The gather operation uses the reshaped indexes to reorder the patches in the sequences tensor."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class PatchShuffle(torch.nn.Module):\n","    def __init__(self, ratio) -> None:\n","        super().__init__()\n","        self.ratio = ratio\n","\n","    def forward(self, patches : torch.Tensor):\n","        T, B, C = patches.shape  # T is the total number of patches\n","        \n","        remain_T = ... # TODO: compute the remaining patches after masking\n","        \n","        # TODO: Generate forward and backward indexes for each batch\n","        indexes = ...\n","        \n","        forward_indexes = torch.as_tensor(np.stack([i[0] for i in indexes], axis=-1), dtype=torch.long).to(patches.device)\n","        backward_indexes = torch.as_tensor(np.stack([i[1] for i in indexes], axis=-1), dtype=torch.long).to(patches.device)\n","        \n","        # Use take_indexes to shuffle the patches\n","        patches = take_indexes(patches, forward_indexes)\n","        patches = ... # TODO Keep only the remaining patches\n","\n","        return patches, forward_indexes, backward_indexes"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Helper Function to reconstruct the image with black patches for masked regions\n","def reconstruct_with_mask(image, patches, original_shape, mask_indexes, patch_size):\n","    B, num_patches, C, P_H, P_W = patches.shape\n","    reconstructed_image = torch.zeros(original_shape).to(image.device)  # Initialize with zeros (black pixels)\n","    \n","    # Unfold the image into patches, fill in only the unmasked ones\n","    patch_num = 0\n","    mask_set = set(mask_indexes.flatten().cpu().numpy())\n","    count = 0\n","\n","    for i in range(0, original_shape[2], patch_size):\n","        for j in range(0, original_shape[3], patch_size):\n","            if count not in mask_set:  # If the patch is not masked\n","                if patch_num < patches.shape[1]:  # Check to avoid index out of bounds\n","                    reconstructed_image[:, :, i:i + patch_size, j:j + patch_size] = patches[:, patch_num]\n","                patch_num += 1\n","            count += 1\n","\n","    return reconstructed_image\n","\n","# Create an example image (48x48x3)\n","# image = torch.rand(1, 3, 48, 48)  # [Batch, Channels, Height, Width]\n","image = next(iter(train_loader))[0]\n","idx = np.random.choice(image.shape[0])  # Randomly select an image from the batch\n","image = image[idx].unsqueeze(0)\n","\n","# Set patch size and create a patch layer\n","PATCH_SIZE = 6  # Extract 6x6 patches\n","patch_layer = Patches(PATCH_SIZE)\n","\n","# Extract patches from the image\n","patches = patch_layer(image)  # [B, num_patches, C, patch_size, patch_size]\n","print(f\"Number of patches: {patches.shape[1]}\")  # Should be 64 patches (48x48 image with 6x6 patches)\n","\n","# Create a PatchShuffle layer with a 25% masking ratio (keep 75% of patches)\n","shuffle_layer = PatchShuffle(ratio=0.75)\n","\n","# Reshape the patches to match the input expected by PatchShuffle\n","# PatchShuffle expects shape [num_patches, batch_size, channels * patch_size * patch_size]\n","B, num_patches, C, P_H, P_W = patches.shape\n","patches_for_shuffling = patches.view(B * num_patches, C * P_H * P_W).unsqueeze(1)  # [num_patches * B, 1, C * patch_size * patch_size]\n","\n","# Shuffle patches and apply masking\n","masked_patches, forward_indexes, backward_indexes = shuffle_layer(patches_for_shuffling)\n","\n","# Reshape the masked patches back to [B, num_patches, C, patch_size, patch_size]\n","remain_T = masked_patches.shape[0]  # Number of remaining patches after masking\n","masked_patches = masked_patches.view(remain_T, B, C, P_H, P_W).permute(1, 0, 2, 3, 4)  # [B, num_patches_remain, C, patch_size, patch_size]\n","\n","# Mask indexes for patches (the ones not used)\n","masked_indexes = forward_indexes[remain_T:]\n","\n","# Reconstruct the original image with black pixels in place of masked patches\n","reconstructed_image_with_mask = reconstruct_with_mask(image, masked_patches, image.shape, masked_indexes, PATCH_SIZE)\n","\n","# Display the original and reconstructed image with masked patches\n","fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n","ax[0].imshow(image.squeeze(0).permute(1, 2, 0).cpu().numpy())  # Original image\n","ax[0].set_title('Original')\n","ax[0].axis(\"off\")\n","\n","ax[1].imshow(reconstructed_image_with_mask.squeeze(0).permute(1, 2, 0).cpu().numpy())  # Image with masked patches\n","ax[1].set_title('Masked')\n","ax[1].axis(\"off\")\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## MAE Model\n","\n","Now, let's built our MAE model\n","\n","<!-- ![MAE](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*MbmkubC541LdgIfnseaCQw.png) -->\n","<div style=\"text-align: center;\">\n","<img src=\"https://miro.medium.com/v2/resize:fit:4800/format:webp/1*MbmkubC541LdgIfnseaCQw.png\" width=\"800\">\n","</div>\n","\n","Here we are using:\n","- ViT Tiny\n","- Masking by shuffle approach. See other example approach [here](https://github.com/open-mmlab/mmpretrain/blob/17a886cb5825cd8c26df4e65f7112d404b99fe12/mmpretrain/models/selfsup/mae.py#L108-L150)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","import timm\n","import numpy as np\n","\n","from einops import repeat, rearrange\n","from einops.layers.torch import Rearrange\n","\n","from timm.models.layers import trunc_normal_\n","from timm.models.vision_transformer import Block\n","\n","\n","class MAE_Encoder(torch.nn.Module):\n","    def __init__(self,\n","                 image_size=32,\n","                 patch_size=2,\n","                 emb_dim=192,\n","                 num_layer=12,\n","                 num_head=3,\n","                 mask_ratio=0.75,\n","                 ) -> None:\n","        super().__init__()\n","\n","        self.cls_token = torch.nn.Parameter(torch.zeros(1, 1, emb_dim))\n","        self.pos_embedding = torch.nn.Parameter(torch.zeros((image_size // patch_size) ** 2, 1, emb_dim))\n","        self.shuffle = PatchShuffle(mask_ratio) # Class we defined above\n","        \n","        # In implementation: Patchify + Projection directly with Conv2D\n","        self.patchify = torch.nn.Conv2d(3, emb_dim, patch_size, patch_size)\n","\n","        self.transformer = torch.nn.Sequential(*[Block(emb_dim, num_head) for _ in range(num_layer)])\n","\n","        self.layer_norm = torch.nn.LayerNorm(emb_dim)\n","\n","        self.init_weight()\n","\n","    def init_weight(self):\n","        trunc_normal_(self.cls_token, std=.02)\n","        trunc_normal_(self.pos_embedding, std=.02)\n","\n","    def forward(self, img):\n","        patches = self.patchify(img)\n","        patches = rearrange(patches, 'b c h w -> (h w) b c')\n","        patches = ... # TODO add positional embeddings to the patches + linear projection\n","\n","        patches, forward_indexes, backward_indexes = ... # TODO apply the masking using the random shuffle approach\n","        \n","        # Add CLS token as in ViT architecture\n","        patches = torch.cat([self.cls_token.expand(-1, patches.shape[1], -1), patches], dim=0)\n","        patches = rearrange(patches, 't b c -> b t c')\n","        features = self.layer_norm(self.transformer(patches))\n","        features = rearrange(features, 'b t c -> t b c')\n","\n","        return features, backward_indexes\n","\n","class MAE_Decoder(torch.nn.Module):\n","    def __init__(self,\n","                 image_size=32,\n","                 patch_size=2,\n","                 emb_dim=192,\n","                 num_layer=4,\n","                 num_head=3,\n","                 ) -> None:\n","        super().__init__()\n","\n","        self.mask_token = torch.nn.Parameter(torch.zeros(1, 1, emb_dim))\n","        self.pos_embedding = torch.nn.Parameter(torch.zeros((image_size // patch_size) ** 2 + 1, 1, emb_dim))\n","        \n","        # Lightweight reconstruction using transformer blocks\n","        self.transformer = torch.nn.Sequential(*[Block(emb_dim, num_head) for _ in range(num_layer)])\n","\n","        self.head = torch.nn.Linear(emb_dim, 3 * patch_size ** 2)\n","        # Other approach similar to reconstruct_from_patch in Patches class using einops\n","        self.patch2img = Rearrange('(h w) b (c p1 p2) -> b c (h p1) (w p2)', p1=patch_size, p2=patch_size, h=image_size//patch_size)\n","\n","        self.init_weight()\n","\n","    def init_weight(self):\n","        trunc_normal_(self.mask_token, std=.02)\n","        trunc_normal_(self.pos_embedding, std=.02)\n","\n","    def forward(self, features, backward_indexes):\n","        T = features.shape[0]\n","        backward_indexes = torch.cat([torch.zeros(1, backward_indexes.shape[1]).to(backward_indexes), backward_indexes + 1], dim=0)\n","        features = torch.cat([features, self.mask_token.expand(backward_indexes.shape[0] - features.shape[0], features.shape[1], -1)], dim=0)\n","        features = take_indexes(features, backward_indexes)\n","        features = features + self.pos_embedding\n","\n","        features = rearrange(features, 't b c -> b t c')\n","        features = self.transformer(features)\n","        features = rearrange(features, 'b t c -> t b c')\n","        features = features[1:] # remove global feature\n","\n","        patches = self.head(features)\n","        mask = torch.zeros_like(patches)\n","        mask[T-1:] = 1\n","        mask = take_indexes(mask, backward_indexes[1:] - 1)\n","        img = self.patch2img(patches)\n","        mask = self.patch2img(mask)\n","\n","        return img, mask\n","\n","class MAE_ViT(torch.nn.Module):\n","    def __init__(self,\n","                 image_size=32,\n","                 patch_size=2,\n","                 emb_dim=192,\n","                 encoder_layer=12,\n","                 encoder_head=3,\n","                 decoder_layer=4,\n","                 decoder_head=3,\n","                 mask_ratio=0.75,\n","                 ) -> None:\n","        super().__init__()\n","        \n","        # TODO: define the encoder and decoder of the MAE\n","        self.encoder = ...\n","        self.decoder = ...\n","\n","    def forward(self, img):\n","        features, backward_indexes = self.encoder(img)\n","        predicted_img, mask = self.decoder(features,  backward_indexes)\n","        return predicted_img, mask\n","\n","\n","if __name__ == '__main__':\n","    shuffle = PatchShuffle(0.75)\n","    a = torch.rand(16, 2, 10)\n","    b, forward_indexes, backward_indexes = shuffle(a)\n","    print(b.shape)\n","\n","    img = torch.rand(2, 3, 32, 32)\n","    encoder = MAE_Encoder()\n","    decoder = MAE_Decoder()\n","    features, backward_indexes = encoder(img)\n","    print(forward_indexes.shape)\n","    predicted_img, mask = decoder(features, backward_indexes)\n","    print(predicted_img.shape)\n","    loss = torch.mean((...) ** 2 * mask / 0.75) # TODO complete the loss\n","    print(loss)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["############################################\n","# Hyperparameters\n","SEED = 42\n","BATCH_SIZE = 4096\n","MAX_DEVICE_BATCH_SIZE=512\n","BASE_LEARNING_RATE=1.5e-4\n","WEIGHT_DECAY=0.05\n","MASK_RATIO=0.75\n","TOTAL_EPOCH=2000\n","WARMUP_EPOCH=200\n","EARLY_STOP=30\n","SAVE_MODEL_PATH='vit-t-mae.pt'\n","############################################"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import random\n","\n","def setup_seed(seed=42):\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    torch.backends.cudnn.deterministic = True"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load TensorBoard extension\n","%load_ext tensorboard\n","\n","# Start TensorBoard\n","%tensorboard --logdir logs/cifar10/mae-pretrain"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","import argparse\n","import math\n","import torch\n","import torchvision\n","from torch.utils.tensorboard import SummaryWriter\n","from torchvision.transforms import ToTensor, Compose, Normalize\n","from tqdm import tqdm\n","\n","\n","setup_seed(SEED)\n","\n","batch_size = BATCH_SIZE\n","load_batch_size = min(MAX_DEVICE_BATCH_SIZE, batch_size)\n","\n","assert batch_size % load_batch_size == 0\n","steps_per_update = batch_size // load_batch_size\n","\n","train_dataset = torchvision.datasets.CIFAR10('data', train=True, download=True, transform=Compose([ToTensor(), Normalize(0.5, 0.5)]))\n","val_dataset = torchvision.datasets.CIFAR10('data', train=False, download=True, transform=Compose([ToTensor(), Normalize(0.5, 0.5)]))\n","dataloader = torch.utils.data.DataLoader(train_dataset, load_batch_size, shuffle=True, num_workers=4)\n","writer = SummaryWriter(os.path.join('logs', 'cifar10', 'mae-pretrain'))\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","model = MAE_ViT(mask_ratio=MASK_RATIO).to(device)\n","optim = torch.optim.AdamW(model.parameters(), lr=BASE_LEARNING_RATE * BATCH_SIZE / 256, betas=(0.9, 0.95), weight_decay=WEIGHT_DECAY)\n","lr_func = lambda epoch: min((epoch + 1) / (WARMUP_EPOCH + 1e-8), 0.5 * (math.cos(epoch / TOTAL_EPOCH * math.pi) + 1))\n","lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optim, lr_lambda=lr_func, verbose=True)\n","\n","step_count = 0\n","optim.zero_grad()\n","for e in range(TOTAL_EPOCH):\n","    model.train()\n","    losses = []\n","    for img, label in tqdm(iter(dataloader)):\n","        step_count += 1\n","        img = img.to(device)\n","        predicted_img, mask = model(img)\n","        loss = torch.mean((...) ** 2 * mask) / MASK_RATIO # TODO: Complete the image loss\n","        loss.backward()\n","        if step_count % steps_per_update == 0:\n","            optim.step()\n","            optim.zero_grad()\n","        losses.append(loss.item())\n","    lr_scheduler.step()\n","    avg_loss = sum(losses) / len(losses)\n","    writer.add_scalar('mae_loss', avg_loss, global_step=e)\n","    print(f'In epoch {e}, average traning loss is {avg_loss}.')\n","\n","    ''' visualize the first 16 predicted images on val dataset'''\n","    model.eval()\n","    with torch.no_grad():\n","        val_img = torch.stack([val_dataset[i][0] for i in range(16)])\n","        val_img = val_img.to(device)\n","        predicted_val_img, mask = model(val_img)\n","        predicted_val_img = predicted_val_img * mask + val_img * (1 - mask)\n","        img = torch.cat([val_img * (1 - mask), predicted_val_img, val_img], dim=0)\n","        img = rearrange(img, '(v h1 w1) c h w -> c (h1 h) (w1 v w)', w1=2, v=3)\n","        writer.add_image('mae_image', (img + 1) / 2, global_step=e)\n","\n","    ''' save model '''\n","    torch.save(model, SAVE_MODEL_PATH)\n","    if EARLY_STOP and e == EARLY_STOP:\n","        print(\"Early stopping (Training takes too long - remove this by setting EARLY_STOP = None) ....\")\n","        break"]},{"cell_type":"markdown","metadata":{},"source":["## Downstream Task"]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"text-align: center;\">\n","    <img src=\"https://i.ibb.co/2FV5YnW/Picture1.png\" width=\"600\">\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class ViT_Classifier(torch.nn.Module):\n","    def __init__(self, encoder : MAE_Encoder, num_classes=10) -> None:\n","        super().__init__()\n","        # We load the components of the MAE Encoder but we don't perform masking\n","        # We don't use the MAE Decoder part anymore, it was useful just for the pretext task\n","        self.cls_token = encoder.cls_token\n","        self.pos_embedding = encoder.pos_embedding\n","        self.patchify = encoder.patchify\n","        self.transformer = encoder.transformer\n","        self.layer_norm = encoder.layer_norm\n","        # We add a classification head to perform the classification\n","        self.head = torch.nn.Linear(self.pos_embedding.shape[-1], num_classes)\n","\n","    def forward(self, img):\n","        patches = self.patchify(img)\n","        patches = rearrange(patches, 'b c h w -> (h w) b c')\n","        patches = patches + self.pos_embedding\n","        patches = torch.cat([self.cls_token.expand(-1, patches.shape[1], -1), patches], dim=0)\n","        patches = rearrange(patches, 't b c -> b t c')\n","        features = self.layer_norm(self.transformer(patches))\n","        features = rearrange(features, 'b t c -> t b c')\n","        logits = self.head(features[0])\n","        return logits"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Let's download the pre-trained ViT MAE since pre-train it from scratch will take too much time even using ViT-Tiny and CIFAR1.0\n","!wget https://github.com/IcarusWizard/MAE/releases/download/cifar10/vit-t-mae.pt\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["############################################\n","# Hyperparameters Downstream Task\n","SEED = 42\n","BATCH_SIZE = 128\n","MAX_DEVICE_BATCH_SIZE=256\n","BASE_LEARNING_RATE=1e-3\n","WEIGHT_DECAY=0.05\n","TOTAL_EPOCH=100\n","WARMUP_EPOCH=5\n","# EARLY_STOP=5\n","PRETRAINED_MODEL_PATH='vit-t-mae.pt' # If empty then train classifier from scratch (traditional supervised approach)\n","OUTPUT_MODEL_PATH='vit-t-mae.pt'\n","############################################"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # Load TensorBoard extension\n","# %load_ext tensorboard\n","\n","# # Start TensorBoard\n","# %tensorboard --logdir logs/cifar10/pretrain-cls"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["setup_seed(SEED)\n","\n","batch_size = BATCH_SIZE\n","load_batch_size = min(MAX_DEVICE_BATCH_SIZE, batch_size)\n","\n","assert batch_size % load_batch_size == 0\n","steps_per_update = batch_size // load_batch_size\n","\n","train_dataset = torchvision.datasets.CIFAR10('data', train=True, download=True, transform=Compose([ToTensor(), Normalize(0.5, 0.5)]))\n","val_dataset = torchvision.datasets.CIFAR10('data', train=False, download=True, transform=Compose([ToTensor(), Normalize(0.5, 0.5)]))\n","train_dataloader = torch.utils.data.DataLoader(train_dataset, load_batch_size, shuffle=True, num_workers=4)\n","val_dataloader = torch.utils.data.DataLoader(val_dataset, load_batch_size, shuffle=False, num_workers=4)\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","if PRETRAINED_MODEL_PATH is not None:\n","    model = torch.load(PRETRAINED_MODEL_PATH, map_location='cpu')\n","    writer = SummaryWriter(os.path.join('logs', 'cifar10', 'pretrain-cls'))\n","else:\n","    model = MAE_ViT()\n","    writer = SummaryWriter(os.path.join('logs', 'cifar10', 'scratch-cls'))\n","model = ViT_Classifier(model.encoder, num_classes=10).to(device)\n","\n","loss_fn = torch.nn.CrossEntropyLoss()\n","acc_fn = lambda logit, label: torch.mean((logit.argmax(dim=-1) == label).float())\n","\n","optim = torch.optim.AdamW(model.parameters(), lr=BASE_LEARNING_RATE * BATCH_SIZE / 256, betas=(0.9, 0.999), weight_decay=WEIGHT_DECAY)\n","lr_func = lambda epoch: min((epoch + 1) / (WARMUP_EPOCH + 1e-8), 0.5 * (math.cos(epoch / TOTAL_EPOCH * math.pi) + 1))\n","lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optim, lr_lambda=lr_func, verbose=True)\n","\n","best_val_acc = 0\n","step_count = 0\n","optim.zero_grad()\n","for e in range(TOTAL_EPOCH):\n","    model.train()\n","    losses = []\n","    acces = []\n","    for img, label in tqdm(iter(train_dataloader)):\n","        step_count += 1\n","        img = img.to(device)\n","        label = label.to(device)\n","        logits = model(img)\n","        loss = loss_fn(logits, label)\n","        acc = acc_fn(logits, label)\n","        loss.backward()\n","        if step_count % steps_per_update == 0:\n","            optim.step()\n","            optim.zero_grad()\n","        losses.append(loss.item())\n","        acces.append(acc.item())\n","    lr_scheduler.step()\n","    avg_train_loss = sum(losses) / len(losses)\n","    avg_train_acc = sum(acces) / len(acces)\n","    print(f'In epoch {e}, average training loss is {avg_train_loss}, average training acc is {avg_train_acc}.')\n","\n","    model.eval()\n","    with torch.no_grad():\n","        losses = []\n","        acces = []\n","        for img, label in tqdm(iter(val_dataloader)):\n","            img = img.to(device)\n","            label = label.to(device)\n","            logits = model(img)\n","            loss = loss_fn(logits, label)\n","            acc = acc_fn(logits, label)\n","            losses.append(loss.item())\n","            acces.append(acc.item())\n","        avg_val_loss = sum(losses) / len(losses)\n","        avg_val_acc = sum(acces) / len(acces)\n","        print(f'In epoch {e}, average validation loss is {avg_val_loss}, average validation acc is {avg_val_acc}.')  \n","\n","    if avg_val_acc > best_val_acc:\n","        best_val_acc = avg_val_acc\n","        print(f'saving best model with acc {best_val_acc} at {e} epoch!')       \n","        torch.save(model, OUTPUT_MODEL_PATH)\n","\n","    writer.add_scalars('cls/loss', {'train' : avg_train_loss, 'val' : avg_val_loss}, global_step=e)\n","    writer.add_scalars('cls/acc', {'train' : avg_train_acc, 'val' : avg_val_acc}, global_step=e)"]},{"cell_type":"markdown","metadata":{},"source":["#### Bonus\n","- Masked Language Modeling Tutorial: https://www.kaggle.com/code/shreydan/masked-language-modeling-from-scratch"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
